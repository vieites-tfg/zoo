\chapter{Manuales de usuario}
\label{chap:usuario}
%
Este Trabajo de Fin de Grado se compone de tres repositorios bien diferenciados y con su propia funcionalidad. Todos ellos se pueden encontrar en esta organización, \href{https://github.com/vieites-tfg}{vieites-tfg}.

\begin{itemize}
  \item \texttt{zoo}: Es el repositorio principal del trabajo. En él se encuentra todo el código fuente, desde la aplicación de prueba hasta la implementación de los módulos de Dagger. Es el único necesario para probar las diferentes funcionalidades.
  \item \texttt{helm-repository}: Alberga las Chart de Helm que describen el despliegue de la aplicación de prueba en Kubernetes.
  \item \texttt{state}: En él se almacenan los valores de las Charts de Helm correspondientes a cada uno de los entornos en los que se puede desplegar la aplicación. Funciona como única fuente de verdad. También es el lugar de donde ArgoCD obtiene los recursos de Kubernetes, con el fin de ser desplegados.
\end{itemize}

Cada uno de los repositorios tiene su propio \texttt{README}. Consultar los demás para más información.

\section{zoo}

\subsection*{Estructura general del repositorio}

\begin{longlisting}
  \begin{minted}{bash}
argo
|- argo_dev.yaml
|- argo_pre.yaml
|- argo_pro.yaml
|- values.yaml
cluster
|- kind_dev.yaml
|- kind_local.yaml
|- kind_pre.yaml
|- kind_pro.yaml
dagger
|- cd
|- ci
docker-compose.yml
Dockerfile
example.env
mongo-init
|- init-zoo.js
packages
|- backend
|- frontend
scripts
|- create_envs.sh
|- image.sh
|- push_package.sh
sops
|- .sops.yaml
|- age.agekey
\end{minted}
\caption{Estructura principal del repositorio ``zoo''}
\end{longlisting}

Este repositorio funciona como un monorepo, lo cual indica que todo el código fuente se encuentra en este único lugar.

\subsubsection*{Aplicación de prueba}

Consta de un \textit{frontend} y un \textit{backend}, ambos creados con Typescript, y utilizando Vue para el \textit{frontend}. Se gestiona utilizando una herramienta de gestión de \textit{monorepos} llamada Lerna. En la raíz del repositorio se pueden encontrar archivos de configuración de Lerna y Node.js. El código correspondiente al \textit{frontend} y al \textit{backend} se incluyen en el directorio \texttt{packages}. El \textit{backend} se conecta a una base de datos de animales de un zoo, y proporciona una API REST que el \textit{frontend} consume, con el fin de poder realizar acciones CRUD sobre la base de datos.

\subsubsection*{Dagger}

En el directorio \texttt{dagger} se encuentran las implementaciones de los módulos correspondientes a los flujos de CI y CD. Estos se pueden ejecutar localmente teniendo Dagger y Docker instalados. Se utilizan en el \textit{workflow} de GitHub encargado de realizar todo el flujo de testeo, publicación y despliegue.

\subsubsection*{Otros}

También se pueden encontrar varios ejecutables en el directorio de \texttt{scripts}, de los cuales \texttt{create\_envs.sh} es el más interesante. Este permite levantar los \textit{clusters} en local para probar el despliegue de la aplicación en los tres entornos posibles: \texttt{dev}, \texttt{pre} y \texttt{pro}.

En todos los \textit{clusters} se instala ArgoCD, aplicación creada específicamente para Kubernetes y que utiliza el método \textit{pull}, siguiendo la filisofía GitOps, leyendo los recursos a desplegar del repositorio de estado (\texttt{state}) mencionado anteriormente. Este tiene una rama \texttt{deploy}, en la que se suben los archivos necesarios para que ArgoCD lea y despliegue la aplicación.

Se configuran los \textit{clusters} y las propias instancias de ArgoCD de manera diferente para cada uno de los clusters. Estas configuraciones se pueden encontrar en los directorios \texttt{cluster} y \texttt{argo}.

Además, este Trabajo de Fin de Grado tiene una memoria asociada, en la cual se pueden encontrar unas claves privada y pública, junto con otros \textit{tokens}, necesarias para poder probar la implementación. Estas claves y \textit{tokens} tienen varios propósitos:

\begin{itemize}
  \item Claves privada y pública: Se utilizan para encriptar y desencriptar los secretos (\texttt{Secrets}) de Kubernetes.
  \item \texttt{tokens}: Un \texttt{CR\_PAT}, que permite acceder al Container Registry de GitHub, donde se encuentran las imágenes de Docker de la aplicación; y un \texttt{STATE\_REPO}, necesario para poder actualizar el repositorio de estado y actualizar los recursos que se van a desplegar.
\end{itemize}

\subsection*{Prueba mínima}

Aquí se describen los requisitos de \textit{software} y pasos a seguir para probar de la manera más simple la aplicación. Con esta prueba se comprobará que se pueden obtener los recursos de Kubernetes y levantar la aplicación en los diferentes entornos, pudiendo visualizar todos estos recursos en ArgoCD.

Los elementos que van a influir en esta prueba serán: los \textit{clusters} junto con sus configuraciones y las de ArgoCD, y el repositorio de estado, en el cual ya debería haber recursos preparados para desplegar.

\subsubsection*{Requisitos de software}

A continuación, en la Tabla \ref{table:software} se indica el software junto con las versiones utilizadas para el desarrollo del proyecto.

Es necesario tener en cuenta que \textit{no} se ha probado en un sistema operativo Windows, por lo que no se asegura su funcionamiento en este. Sí se ha probado en MacOS y distribuciones Linux.

\begin{table}
  \centering
  \begin{tabular}{|l|l|}
    \hline
    \textit{Software} & \textit{Version} \\ \hline
    \href{https://git-scm.com/book/en/v2/Getting-Started-Installing-Git}{Git} & 2.48.1 \\ \hline
    \href{https://github.com/casey/just?tab=readme-ov-file#installation}{Just} & 1.39.0 \\ \hline
    \href{https://docs.docker.com/desktop/}{Docker} & 27.5.1 \\ \hline
    \href{https://kubernetes.io/docs/tasks/tools/#kubectl}{Kubectl} & 1.33 \\ \hline
    \href{https://kubernetes.io/docs/tasks/tools/#kind}{Kind} & 0.27.0 \\ \hline
    \href{https://helm.sh/docs/intro/install/}{Helm} & 3.17.3 \\ \hline
  \end{tabular}
  \caption{Software y versiones utilizadas durante el desarrollo}
  \label{table:software}
\end{table}

\subsubsection*{Cómo probar}

\begin{enumerate}
  \item Clonar este repositorio.

    Se clona el repositorio y se accede al directorio.

\begin{listing}[!ht]
  \begin{minted}{bash}
git clone https://github.com/vieites-tfg/zoo ~/zoo
cd ~/zoo
\end{minted}
\caption{Clonado y acceso al repositorio}
\end{listing}

\item Clave privada.

\begin{listing}[!ht]
  \begin{minted}{bash}
mkdir -p ./sops
echo "AGE-SECRET-KEY-1CTS4S4QNNZ9N9YXXM288LSE9VKPJ220E57ZHC4558WMZ8LG2QWKQFFER8C" > ./sops/age.agekey
\end{minted}
\caption{Almacenamiento de la clave privada de encriptado}
\end{listing}

  \item Ejecutar el script de creación de los entornos.

\begin{listing}[!ht]
  \begin{minted}{bash}
./scripts/create_envs.sh
\end{minted}
\caption{Script de creación de los entornos}
\end{listing}

    El script anterior:
    \begin{itemize}
      \item Crea tres clusters (`dev`, `pre` y `pro`), con tres contextos diferentes (`kind-{{cluster}}`), con su propia configuración.
      \item Introduce la clave privada, creada previamente, en cada uno de los clusters, para permitir a ArgoCD desencriptar los secretos.
      \item Instala ArgoCD en cada uno de los clusters, con sus respectivas configuraciones, obteniendo cada uno los recursos de despliegue del entorno que le toca.
    \end{itemize}

  \item Acceso a los clusters.

    A medida que se van creando los clusters, las contraseñas del usuario \texttt{admin} de ArgoCD se van mostrando. También se muestran todas al finalizar la ejecución del script.

    Para poder acceder a cada uno de los clusters, lo primero que hay que hacer es mapear un puerto local libre al puerto 443 del servidor de ArgoCD. Esto se consigue de la siguiente manera:

\begin{listing}[!ht]
  \begin{minted}{bash}
kubectl port-forward svc/argocd-server -n argocd --context kind-{{cluster}} 8086:443
\end{minted}
\caption{Disponer un puerto en local para acceder a ArgoCD}
\end{listing}

    En el anterior comando, hay que cambiar \texttt{\{\{cluster\}\}} por aquel al que se quiera acceder. Se podrá acceder a ArgoCD a través del navegador en \texttt{localhost:8086}

    Se pide usuario y contraseña para entrar, que son \texttt{admin} y la contraseña de dicho cluster, mostrada en la salida del script que se ha ejecutado antes.

  \item Acceder a la aplicación. (opcional)

    Lo primero que hay que hacer es configurar los hosts del ordenador para que se resuelvan las rutas como \texttt{localhost}. Para ello se puede ejecutar el siguiente comando:

\begin{listing}[!ht]
  \begin{minted}{bash}
just check_hosts dev pre pro
\end{minted}
\caption{Configuración del host para acceder a las URLs de la aplicación}
\end{listing}

    El comando anterior modifica el archivo \texttt{/etc/hosts}, incluyendo las líneas nesecarias para poder resolver las rutas de acceso a la aplicación. Es necesario tener permisos de usuario o poner la contraseña de este en el caso de que se pida.

    Hay que tener en cuenta que cada entorno tiene su propio puerto, que son:
    \begin{itemize}
      \item \texttt{dev}: 8080
      \item \texttt{pre}: 8081
      \item \texttt{pro}: 8082
    \end{itemize}

    Los anteriores puertos se podrían modificar, pero sería necesario actualizar tanto las configuraciones de los clusters en \texttt{zoo/cluster} como los valores de los puertos del Ingress en el repositorio de estado \texttt{state} para cada uno de los entornos. Esto implicaría tener que hacer un nuevo despliegue con el fin de actualizar los valores en los propios recursos de Kubernetes. No sería necesario publicar nuevas imágenes.

    Ahora se puede acceder a la aplicación de gestión del zoo. a través de las siguientes rutas en el navegador:

    \begin{itemize}
      \item \textit{frontend}: \texttt{zoo-\{\{entorno\}\}.example.com:\{\{puerto\_entorno\}\}}
      \item \textit{backend}: \texttt{api-zoo-\{\{entorno\}\}.example.com:\{\{puerto\_entorno\}\}}
    \end{itemize}

\end{enumerate}

\subsubsection*{Conclusión}

Se comprueba que funciona el despliegue de la aplicación para cualquiera de los entornos. Esta configuración permite al desarrollador tener a su disposición cada una de las versiones y comprobar que el despliegue se realiza correctamente. El uso de ArgoCD y la capacidad de obtener los recursos de una única fuente de verdad hace de este flujo de despliegue algo esencial en cualquier equipo de desarrollo.

Es necesario mencionar que, en entornos de producción reales, los clusters se encontrarían en la nube. Sin embargo, se podría mantener el cluster de desarrollo \texttt{dev}, con el fin de realizar implementaciones y probar su funcionamiento sin depender de herramientas remotas.

A continuación se indica cómo probar los módulos de Dagger.

\subsection*{Prueba de Dagger}

En este caso se va mostrar cómo probar tanto los módulos de Dagger como el workflow completo de CI/CD, desde la simulación de implementación de una nueva característica en la aplicación, pasando por su despliegue en todos los entornos.

\subsubsection*{Requisitos de software}

Incluyendo los de la prueba anterior.

\begin{table}
  \centering
  \begin{tabular}{|l|l|}
    \hline
    \textit{Software} & \textit{Version} \\ \hline
    \href{https://docs.dagger.io/install/}{Dagger} & latest \\ \hline
    \href{https://nektosact.com/installation/index.html}{act} & 0.2.79 \\ \hline
  \end{tabular}
\end{table}

Para la simulación de una nueva \textit{feature} se utilizar \textit{act}, herramienta que permite ejecutar workflows de GitHub de manera local. Con ella se simulará el disparo de los eventos que hac que el workflow se ejecute, y se comportará de la misma manera que haría en remoto.

\subsubsection*{Cómo probar}

\begin{enumerate}
  \item Creación de archivos `.env` y `.secrets.yaml`

    Es necesario modificar los archivos `example.env` y `example.secrets.yaml` con los datos pertinentes. Los archivos quedarían como se muestra en los Listings \ref{lst:env-file} y \ref{lst:secrets-file}.
  
\begin{listing}[!ht]
  \begin{minted}{bash}
# Se ha divido en dos STATE_REPO, pero es un solo string
MONGO_DATABASE=zoo
MONGO_ROOT=carer
MONGO_ROOT_PASS=carerpass
CR_PAT=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
STATE_REPO=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
\end{minted}
\caption{Archivo de secretos .env}
\label{lst:env-file}
\end{listing}

\begin{listing}[!ht]
  \begin{minted}{yaml}
# Se han divido en dos STATE_REPO, age y SOPS_PRIVATE_KEY,
# pero son un solo string
MONGO_DATABASE: zoo
MONGO_ROOT: carer
MONGO_ROOT_PASS: carerpass
CR_PAT: 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
STATE_REPO: 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
SOPS_CONFIG_FILE: |
  creation_rules:
      - path_regex: ".*\\.ya?ml$"
        unencrypted_regex: "^(apiVersion|metadata|kind|type)$"
        age: age15peyc7pedj8gjqwnarat6s3u87wy4j5xtf7t96vuj74m3l9xq5ys0r4sag
SOPS_PRIVATE_KEY: AGE-SECRET-KEY-1CTS4S4QNNZ9N9YXXM288LSE9VKPJ220E57ZHC4558WMZ8LG2QWKQFFER8C
\end{minted}
\caption{Archivo de secretos .secrets.yaml, para pruebas con act}
\label{lst:secrets-file}
\end{listing}

    Se puede copiar el contenido anterior y pegarlo en su respectivo archivo. Posteriormente, hay que cambiar el nombre de estos, eliminando la parte de `example`.

\begin{listing}[!ht]
  \begin{minted}{bash}
mv example.env .env
mv example.secrets.yaml .secrets.yaml
\end{minted}
\caption{Cambio de nombre de archivos ocultos}
\end{listing}

  \item Prueba local del módulo de CI.

    El módulo de CI de Dagger se divide en funciones para \textit{frontend} y para \textit{backend}, por separado. Las funciones son las mismas para ambos, pero internamente se comportan diferente.

    En la Figura \ref{fig:ci_schema} se muestra un diagrama de la implementación de este módulo.

\begin{figure}
  \centerline{\includegraphics[width=13.5cm]{figuras/ci_schema}}
  \caption{Estructura del módulo de CI.}
  \label{fig:ci_schema}
\end{figure}

    En el diagrama se ve:
    \begin{itemize}
      \item \texttt{CI}: La estructura principal, con cinco funciones, dos de las cuales permiten acceder a las estructuras de \textit{frontend} y \textit{backend}, por separado.
      \item \texttt{Frontend} y \texttt{Backend}: Estructuras dedicadas, con implementación diferente para cada una de las funciones que proporcionan, que en este caso son las mismas para ambos.
    \end{itemize}

    Para poder utilizar el comando \texttt{dagger}, es necesario estar en un directorio de trabajo en el que exista un módulo de Dagger, o bien proporcionarlo con la opción \texttt{-m}. Para facilitar la explicación, se ejecutarán los comandos desde el directorio correspondiente al módulo de CI.

\begin{listing}[!ht]
  \begin{minted}{bash}
cd dagger/ci
\end{minted}
\caption{Acceder al módulo de Dagger de CI.}
\end{listing}

Se pueden obtener las funciones de \texttt{CI}, junto con los campos de la estructura, con el comando:

\begin{listing}[!ht]
  \begin{minted}{bash}
dagger functions
\end{minted}
\caption{Comprobación de las funciones disponibles con Dagger.}
\end{listing}

    Para conocer las funciones y campos de las demás estructuras se ejecuta:

\begin{listing}[!ht]
  \begin{minted}{bash}
dagger call backend --help
dagger call frontend --help
\end{minted}
\caption{Obtener las funciones de cada uno de los objetos customizados del módulo de Dagger de CI.}
\end{listing}

    \texttt{CI} tiene un parámetro requerido, que se trata del archivo \texttt{.env} que se ha creado anteriormente. A continuación se muestra cómo se ejecutarían los tests \textit{end-to-end} de la aplicación, teniendo en cuenta que hay que encontrarse en el directorio del módulo y que el archivo \texttt{.env} se ha creado correctamente en la raíz del repositorio:

\begin{listing}[!ht]
  \begin{minted}{bash}
dagger call --sec-env=file://../../.env endtoend
\end{minted}
\caption{Ejecutar la función de prueba íntegra de los paquetes de la aplicación con Dagger.}
\end{listing}

    La primera vez que se ejecuta el comando anterior puede tardar alrededor de 10 minutos. Siempre dependiendo de la conexión a Internet que se tenga. La segunda vez, ese tiempo debería reducirse considerablemente, alrededor de un 40\%, tardando así 6 minutos. Este tiempo puede reducirse más, a medida que se realizan ejecuciones del mismo, hasta alrededor de un 60\% del tiempo inicial.

    Otro ejemplo sería, levantar el frontend y el backend y hacer que se comuniquen de manera local.

\begin{listing}[!ht]
  \begin{minted}{bash}
dagger call --sec-env=file://../../.env backend service up --ports 3010:3000
dagger call --sec-env=file://../../.env frontend service up --ports 8090:80
\end{minted}
\caption{Levantamiento de los servicios de los paquetes de la aplicación con Dagger.}
\end{listing}

    Simplemente, ejecutando los comandos anteriores en terminales diferentes, los servicios serán capaces de comunicarse. Estos estarán disponibles en \texttt{localhost:\{\{puerto\}\}}. Para acceder a la API se añade la ruta \texttt{/animals}.

    En los comandos anteriores, \texttt{up} es una función del tipo Service, propio de Dagger, que se devuelve en la función \texttt{service}, como se muestra en el diagrama anterior. \texttt{--ports} es un argumento de la función \texttt{up}.

    Otros ejemplos de comandos:

\begin{listing}[!ht]
  \begin{minted}{bash}
dagger call --sec-env=file://../../.env [backend|frontend] lint
dagger call --sec-env=file://../../.env [backend|frontend] test
dagger call --sec-env=file://../../.env [backend|frontend] publish-image --tag "{{tag}}"
dagger call --sec-env=file://../../.env [backend|frontend] publish-pkg
\end{minted}
\caption{Otras posibles funciones a ejecutar del módulo de CI con Dagger.}
\end{listing}

  \item Prueba local del módulo de CD.

    El módulo de CD tiene la estructura que se muestra en la Figura \ref{fig:cd_schema}.

\begin{figure}
  \centerline{\includegraphics[width=13.5cm]{figuras/cd_schema}}
  \caption{Estructura del módulo de CD.}
  \label{fig:cd_schema}
\end{figure}

    La función principal de este módulo es \texttt{deploy}, que se encarga de:
    \begin{itemize}
      \item Construir todos los recursos de Kubernetes, haciendo uso de los repositorios \texttt{helm-repository} y \texttt{state}.
      \item Encriptar los secretos.
      \item Subir los cambios al repositorio de estado.
    \end{itemize}

    Para conseguir esto, se hace uso de \href{https://daggerverse.dev/mod/github.com/prefapp/daggerverse/kind@42985961eb3d61fa98aa71d2f67922a933b5caa3}{este módulo}, que permite crear un \textit{cluster} de KinD.

    No se recomienda la ejecución en local de este módulo, pero se explica igual cómo se haría.

    Para realizar un despliegue habría que ingresar al directorio del módulo y ejecutar el comando que se muestra en el Listing \ref{lst:justcd}:

\begin{longlisting}
  \begin{minted}{bash}
cd ~/zoo/dagger/cd

just deploy dev
# just deploy pre
# just deploy pro

# lo anterior ejecuta:
dagger call \
    --socket=/var/run/docker.sock \
    --kind-svc=tcp://localhost:3000 \
    --config-file=file://../../cluster/kind_local.yaml \
    launch \
    --sec-env=file://../../.env \
    --env={{env}} # <- cambiar {{env}} por "dev", "pre" o "pro" \
    --age-key=file://../../sops/age.agekey \
    --sops-config=file://../../sops/.sops.yaml
  \end{minted}
  \caption{Despliegue con el módulo de Dagger de CD.}
  \label{lst:justcd}
\end{longlisting}


  \item Prueba con \texttt{act}.

    \texttt{act} es una herramienta que permite ejecutar workflows de GitHub en local, pudiendo indicar el \textit{trigger} que dispara el workflow. 

    De esta manera, se puede probar cómo sería el flujo de ejecución en el caso de que se introdujera en la rama principal una nueva característica de la aplicación.

    El workflow encargado de realizar las operaciones de CI y CD es el que se encuentra en \texttt{.github/workflows/cicd.yaml}.

    La definición de los objetos del \textit{trigger} \texttt{release} se encuentran en el directorio \texttt{.github/workflows/events}.

    Una vez se tenga \texttt{act} instalado, se puede iniciar la \textit{promoción de entornos}.

    Antes de nada, para comprobar que se ha realizado el cambio, se realizarán los siguientes pasos para asegurarse de que las \textit{tags} de la imagen de \texttt{dev} es nueva:

\begin{listing}[!ht]
  \begin{minted}{bash}
git switch -c environments-test
  \end{minted}
  \caption{Creación de una nueva rama con Git.}
\end{listing}

    El comando anterior creará una rama nueva con el nombre \texttt{environments-test}.

    Ahora se puede cambiar el título de la página web. Este se encuentra en el archivo \texttt{packages/frontend/src/components/MainTitle.vue}, de ``Zoo'' a algo como ``Mi zoo''.

    Se debe crear un \textit{commit} para que la imagen use como \textit{tag} los ocho primeros caracteres de este:

\begin{listing}[!ht]
  \begin{minted}{bash}
git add .
git commit -m 'cambio de nombre'
  \end{minted}
  \caption{Creación del commit del cambio realizado en el código.}
\end{listing}

    Así se utilizará el \textit{commit} anterior para generar el nombre de la \textit{tag} de las imágenes.

    \textbf{dev}

    Ahora se puede subir todo al entorno de ``dev'', simulando un \texttt{push} (\textit{trigger} por defecto de \texttt{act}) a la rama principal, como si el cambio anterior se hubiera realizado en una PR y si hubieran aprobado los cambios. Esto se puede hacer, desde la raíz del repositorio, con el comando del Listing \ref{lst:actpush}.

\begin{listing}[!ht]
  \begin{minted}{bash}
# se utiliza el archivo .secrets.yaml creado anteriormente
act --secret-file .secrets.yaml
  \end{minted}
  \caption{Simulación de push a la rama principal con act.}
\end{listing}

    Una vez terminada la ejecución, si se tiene levantado el cluster de \texttt{dev}, se debería poder sincronizar ArgoCD. Para comprobar que se ha actualizado la imagen correctamente, se puede visualizar la \textit{tag} que está en uso en el Deployment del frontend \texttt{zoo-dev-frontend}, y buscar el campo \texttt{spec.template.spec.containers.image}, en la pestaña de ``\textit{Live manifest}''. Este campo debería contener la nueva imagen generada.

    También se puede entrar en la propia web, en \texttt{zoo-dev.example.com:8080}, para ver el nuevo título.

    \textbf{pre}

    Para pasar los cambios que se han realizado al entorno de \texttt{pre}, sería neceario crear una \texttt{pre-release}. Se va a simular su creación utilizando la configuración de dicho evento, que se encuentra en \texttt{.github/events/prerelease.json}.

    Lo mejor para comprobar que se actualiza correctamente, es cambiar el nombre de la \texttt{pre-release}, en el campo \texttt{name} del archivo del evento. Por ejemplo, \texttt{0.0.123-snapshot}.

    Con el comando del Listing \ref{lst:actrelease} se realizará la simulación.

\begin{listing}[!ht]
  \begin{minted}{bash}
act release --eventpath .github/events/prerelease.json --secret-file .secrets.yaml
  \end{minted}
    \caption{Simulación de creación de una prerelease con act.}
  \label{lst:actrelease}
\end{listing}

    El archivo de comprobación del recurso es, en este caso, \texttt{zoo-pre-frontend}, y el mismo campo que antes.

    La url sería \texttt{zoo-dev.example.com:8081}.

    \textbf{pro}

    Ahora, es necesario realizar algo parecido al paso anterior. Se modifica el archivo \texttt{.github/events/release.json}, y se pone el mismo \texttt{name} que anter, pero sin la coletilla \texttt{snapshot}, es decir, \texttt{0.0.123}.

    Se simula el cambio al entorno de \texttt{pro}.

\begin{listing}[!ht]
  \begin{minted}{bash}
act release --eventpath .github/events/release.json --secret-file .secrets.yaml
  \end{minted}
    \caption{Simulación de creación de una release con act.}
\end{listing}

\end{enumerate}

\subsubsection*{Conclusión}

Se puede comprobar que el ciclo completo funciona correctamente, y la flexibilidad que da poder ejecutar los módulos de Dagger en local. Esto facilita el desarrollo de características nuevas para la aplicación sin miedo a que se produzca un fallo durante la ejecución del workflow de GitHub en la nube. Así, el resultado que va a dar la ejecución de dicho workflow ya se sabe de antemano, porque se ha probado de manera local, en un entorno controlado gracias a Docker. Por lo tanto, se sabe que este se va a comportar de la misma manera en cualquier otro sistema.

\subsubsection*{Prueba la API}

La API está disponible en \texttt{https://api-\string{\string{env\string}\string}-zoo.example.com/animals}. Tiene definidos los \textit{endpoints} de la Tabla \ref{tabla:endpoints}.

\begin{table}[h]
  \centering
  \begin{tabular}{|l|l|l|}
    \hline
    \textit{Acción} & \textit{endpoint} & \textit{Funcionalidad} \\ \hline
    \texttt{GET} & \texttt{/animals} & Obtener todos los animales \\ \hline
    \texttt{GET} & \texttt{/animals/\{id\}} & Obtener un animal mediante su ID \\ \hline
    \texttt{POST} & \texttt{/animals} & Añadir un nuevo animal \\ \hline
    \texttt{PUT} & \texttt{/animals/\{id\}} & Actualizar el animal con cierto ID \\ \hline
  \end{tabular}
  \caption{Endpoints de la API}
  \label{tabla:endpoints}
\end{table}

En los siguientes Listings se muestran peticiones de ejemplo que se pueden utilizando, por ejemplo, \href{https://www.postman.com/}{Postman}.

\begin{listing}[!ht]
  \begin{minted}{js}
// GET http://localhost:3000/animals
  \end{minted}
  \caption{API REST, obtener la lista de animales.}
\end{listing}

\begin{listing}[!ht]
  \begin{minted}{js}
// GET http://localhost:3000/animals/{{ID_FROM_LAST_LIST}k
  \end{minted}
  \caption{API REST, obtener animal.}
\end{listing}

\begin{listing}[!ht]
  \begin{minted}{js}
// POST http://localhost:3000/animals/
{
    name: "Marcus",
    species: "Tiger",
    birthday: "2010-05-16",
    genre: "male",
    diet: "Carnivore",
    condition: "Healthy"
}
  \end{minted}
  \caption{API REST, crear un animal.}
\end{listing}

\begin{listing}[!ht]
  \begin{minted}{js}
// PUT http://localhost:3000/animals/{{ID_FROM_LAST_POST}}
{
    condition: "Injured",
    notes: "Recovering from minor foot injury."
}
  \end{minted}
  \caption{API REST, actualizar un animal.}
\end{listing}

\subsection{helm-repository}

Este repositorio forma parte de un TFG, donde el repositorio principal es [zoo](http://github.com/vieites-tfg/zoo).

En este reposito se albergan las Charts Helm correspondientes a la aplicación que se implementa en el repositorio principal `zoo`.

En la Figura \ref{fig:charts_diagram} se muestra un diagrama de la disposición de las Charts.

\begin{figure}
  \centerline{\includegraphics[width=13.5cm]{figuras/charts_diagram}}
  \caption{Estructura del módulo de CD.}
  \label{fig:charts_diagram}
\end{figure}

Como se puede comprobar, existe una \textit{Chart Umbrella} \texttt{zoo}, la cual se encarga de indicar las subcharts a crear. Entre estas hay dos creadas específicamente para la aplicación, \texttt{zoo-backend} y \texttt{zoo-frontend}, perfectamente autodescritas. Por último está la Chart de MongoDB, obtenida del repositorio de \href{https://bitnami.com/stacks?stack=helm}{Bitnami}.

\subsubsection*{Estructura del repositorio}

\begin{longlisting}
  \begin{minted}{bash}
helm
|- zoo-0.0.0.tgz
|- zoo-0.0.1.tgz
|- zoo-0.0.2.tgz
|- zoo-0.0.3.tgz
zoo
|- Chart.yaml
|- charts
|   |- zoo-backend
|   |   |- Chart.yaml
|   |   |- templates
|   |       |- configMap.yaml
|   |       |- deployment.yaml
|   |       |- ingress.yaml
|   |       |- secret.yaml
|   |       |- service.yaml
|   |
|   |- zoo-frontend
|       |- Chart.yaml
|       |- templates
|           |- configmap-js.yaml
|           |- configmap.yaml
|           |- deployment.yaml
|           |- ingress.yaml
|           |- service.yaml
|   
|- templates
    |- _helpers.tpl
    |- ghcr-secret.yaml
  \end{minted}
  \caption{Estructura del repositorio \texttt{helm-repository}}
\end{longlisting}

\subsection{state}

En este repositorio se almacenan los valores necesarios para las Charts de Helm correspondientes a la aplicación, que se encuentran en \href{https://github.com/vieites-tfg/helm-repository}{helm-repository}.

\subsubsection*{Estructura del repositorio}

\begin{listing}[!ht]
  \begin{minted}{bash}
dev
|- global.yaml
|- mongodb.yaml
|- zoo-backend.yaml
|- zoo-frontend.yaml
pre
|- global.yaml
|- mongodb.yaml
|- zoo-backend.yaml
|- zoo-frontend.yaml
pro
|- global.yaml
|- mongodb.yaml
|- zoo-backend.yaml
|- zoo-frontend.yaml
dev.yaml
global.yaml
helmfile.yaml.gotmpl
mongodb.yaml
pre.yaml
pro.yaml
zoo-backend.yaml
zoo-frontend.yaml
  \end{minted}
  \caption{Estructura del repositorio \texttt{state}.}
\end{listing}

Se ven los siguientes archivos:
\begin{itemize}
  \item \texttt{global}: En estos se indican valores globales a toda la aplicación.
  \item \texttt{zoo-backend}, \texttt{zoo-frontend} y \texttt{mongodb}: Archivos de valores de cada una de las Subcharts.
  \item \texttt{helmfile.yaml.gotmpl}: Archivo en el que se indican los repositorios a utilizar y los valores a incluir en las Charts. Se utiliza con la herramienta \href{https://helmfile.readthedocs.io/en/latest/}{helmfile} para generar la plantilla de todos los recursos que se van a construir.
\end{itemize}

Existen directorios para cada uno de los posibles entornos, con valores específicos de dicho entorno para cada una de las Subcharts o valores globales.

\subsubsection*{Rama de despliegue}

La rama de despliegue \texttt{deploy} es la rama en la que se publican los recursos que ArgoCD va a leer para construir la aplicación.

\textbf{Estructura}

\begin{listing}[!ht]
  \begin{minted}{bash}
dev
|- kustomization.yaml
|- non-secrets.yaml
|- secret_generator.yaml
|- secrets.yaml
pre
|- kustomization.yaml
|- non-secrets.yaml
|- secret_generator.yaml
|- secrets.yaml
pro
|- kustomization.yaml
|- non-secrets.yaml
|- secret_generator.yaml
|- secrets.yaml
  \end{minted}
  \caption{Estructura de la rama \texttt{deploy} en \texttt{state}.}
\end{listing}

% Se puede comprobar que hay un directorio con los recursos correspondientes de cada uno de los entornos posibles.
%
% En ellos encontramos:
%
% - `non-secrets`: Los recursos de Kubernetes que no son secretos.
% - `secrets`: Los recursos de Kubernetes que son secretos, encriptados utilizando [SOPS](https://github.com/getsops/sops) y [age](https://github.com/FiloSottile/age).
% - `kustomization`: Archivo que lee la herramienta [kustomize](https://kustomize.io/) e indica los recursos y el generador a utilizar para crear los secretos desencriptados.
% - `secret_generator`: Indica la necesidad del uso de la herramienta [ksops](https://github.com/kubernetes/kops) con el fin de desencriptar los secretos.
%
