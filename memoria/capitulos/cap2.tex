\chapter{Estado del arte y fundamentos teóricos}

Antes de empezar a escribir código, se deben entender los conceptos fundamentales que permitirán llevar a cabo este trabajo.

Lo que se intenta mejorar utilizando Dagger es el ciclo completo de CI/CD de una aplicación. Por lo tanto, es fundamental definir los conceptos de \textit{Continuous Integration} y el \textit{Continuous Delivery}. Una vez se comprenda a qué se refieren esos términos, se podrán entender los métodos y tecnologías convencionales que permiten implementar dichos procesos. Será entonces cuando se pueda introducir Dagger, un método innovador para realizar \textit{pipelines}.

\section{CI/CD}

CI/CD son las siglas de \textit{Continuous Integration}/\textit{Continuous Delivery}, o en casos más específicos, este último también se puede conocer como \textit{Continuous Deployment}.

Se trata de un conjunto de pasos automatizados, utilizados en el desarrollo de software para llevar el código desde su implementación inicial hasta el despliegue de la aplicación. Estos pasos incluyen:

\begin{itemize}
  \item Integración de cambios en el código.
  \item Compilación de la aplicación con los cambios realizados.
  \item Realización de pruebas.
  \item Creación y publicación de imágenes de Docker y paquetes NPM.
  \item Despliegue de la aplicación.
\end{itemize}

\subsection*{\textit{Continuous Integration}}
\label{subsec:CI}

\begin{figure}
  \centerline{\includegraphics[width=10cm]{figuras/ci}}
  \caption{Proceso de integración continua.\cite{img:ci}}
  \label{fig:ci}
\end{figure}

Se basa en la integración de código de manera constante, día a día, en un repositorio compartido por programadores. Cada uno de los programadores realiza cambios en el código y lo integra en el repositorio. Una vez se realizan cambios, estos deben pasar una serie de pruebas para que se incluyan definitivamente en el código fuente de la aplicación (Fig. \ref{fig:ci}).

Desde hace años se utiliza un sistema de control de versiones para gestionar el código de cualquier proyecto. Este tipo de herramientas permite a un equipo controlar el estado del código en cada momento, siendo capaces de conocer el historial de los cambios realizados, saber quién ha hecho cada cambio y tener la capacidad de revertir alguna modificación en el caso de ser necesario. La herramienta de control de versiones más utilizada hoy en día, y la que se utiliza en este proyecto, es Git\cite{git}.

La integración de código en un repositorio no se trata simplemente de modificar una porción de un archivo y subirlo. El código debe ser probado antes de incluirlo completamente en el núcleo de la aplicación. Durante el proceso de integración continua, cada vez que se modifica algo de código, se debe:

\begin{itemize}
  \item Construir la aplicación.
  \item Pasar pruebas de funcionalidad.
  \item Pasar a través de un análisis del propio código (\textit{linting}).
  \item Reportar cualquier error en el caso de que exista.
\end{itemize}

Todo lo anterior se debe realizar de manera automatizada, con el fin de integrar el código modificado en el repositorio lo más rápido posible.

\subsection*{\textit{Continuous Delivery}}

\begin{figure}
  \centerline{\includegraphics[width=17cm]{figuras/cd}}
  \caption{Proceso de despliegue continuo.\cite{img:cd}}
  \label{fig:cd}
\end{figure}

Tras haber construido la aplicación durante el proceso de integración continua, toca desplegar la aplicación. El despliegue automático de nuevas versiones de una aplicación que han pasado el ciclo de CI se conoce como ``despliegue continuo''.

Esto, evidentemente, tiene como requisito que la aplicación que se está construyendo tenga el despliegue como uno de los pasos en su ciclo de vida, lo cual no tiene por qué ser así. En este trabajo sí que ocurre, ya que la aplicación \textit{dummy} que se construye es una página web, junto con una API y una base de datos.

Es necesario que exista relación entre los desarrolladores y los encargados de desplegar la aplicación. Sin embargo, hoy en día encontramos el rol DevOps\cite{devops} en muchas empresas, lo cual implica que la persona que ocupa este rol debe tener conocimiento tanto del desarrollo de la aplicación como del despliegue de la misma.

Esta transición a la cultura DevOps permite a los equipos desplegar sus aplicaciones más fácilmente. Además, incluye la necesidad de que el despliegue sea una parte muy importante en el proceso de desarrollo.

Al igual que en la integración continua, en este ciclo también es necesario automatizar el proceso despliegue de una aplicación. Esto siempre va a disminuir la posibilidad de error humano.

Con el despliegue continuo podemos tener \textit{feedback} más rápido por parte del usuario, lo que permitirá mejorar y corregir errores más rápidamente. Además, se despliegan con más frecuencia cambios realizados en la aplicación, por lo que los errores en producción son menos probables y, en el caso de que los haya, más fáciles de corregir. Esto es gracias también a llevar un historial de los cambios mediante una herramienta de control de versiones como Git.

\subsection*{GitOps \& ArgoCD}

Basándose en la filosofía DevOps, mencionada antes, que abarca tanto el ciclo de CI como el de CD, existen un conjunto de prácticas en las que se utiliza Git como fuente de verdad para la gestión de la infraestructura y las aplicaciones. Esto es conocido como GitOps\cite{gitops}. Utilizar Git como la única fuente de información permite gestionar de manera consistente la infraestructura de las aplicaciones. Los cambios en un repositorio hacen que se ejecuten \textit{workflows}, o secuencias de acciones de CI/CD que implementan los cambios en el entorno correspondiente.

La herramienta que realiza estas prácticas de GitOps, y que se utiliza en este trabajo, es ArgoCD\cite{argocd}. Argo es una herramienta que, como su nombre indica, facilida la entrega continua. Está diseañada específicamente para Kubernetes. Automatiza el despliegue de las aplicaciones, superevisando repositorios de Git para aplicar los cambios que se realizan en ellos automáticamente en el \textit{cluster}. Su uso permite no tener que realizar actualizaciones manuales en entornos de producción, y hace que el entorno siempre esté sincronizado con el código definido en el repositorio.

\section{Ecosistema de herramientas}

Un \textit{pipeline} moderno se compone de diferentes tipos de herramientas, cada una con sus características y finalidades. Se pueden agrupar en los ciclos que se han indicado anteriormente, CI y CD. El grupo de herramientas de CI facilitan la construcción y empaquetado de la aplicación que se va a construir, mientras que las de CD permiten la aplicación empaquetada previamente.

\subsection*{Herramientas de construcción y empaquetado}

Como cabe esperar, los pasos mencionados en \ref{subsec:CI}, que forman parte de la integración continua, van a depender del tipo de aplicación que se esté construyendo, y de las tecnologías que se estén utilizando. Además, esta secuencia de acciones pueden incluir unos pocos comandos en trabajos o proyectos sencillos, o necesitar varios \textit{scripts} complejos en el caso de aplicaciones más avanzadas. Por lo tanto, es necesario tener una herramienta de construcción que permita realizar los pasos mencionados anteriormente, sin la necesidad de memorizar cada uno de los comandos o \textit{scripts} que hay que ejecutar.

\subsubsection*{Make}

Para ello existe \texttt{make}\cite{make}, una aplicación de línea de comandos que permite definir bloques de comandos o reglas, aportando a cada bloque un nombre u objetivo que se pretende obtener ejecutando dicha regla. Se suele crear un archivo llamado \texttt{Makefile} para definir todas las reglas que se precisen.

\begin{lstlisting}[language=make,label=lst:make]{Makefile para compilación de un programa en C}
# Compiler
CC = gcc

# Compiler options
CFLAGS = -Wall -g

# Final executable name
TARGET = my_program

# The object files (.o) needed by the program
# Make infers automatically that .o depends on the corresponding .c
OBJS = main.o hello.o

# --- Rules ---

# The first rule is the one executed by default with "make"
# It declares that to create the TARGET, it needs the OBJS
$(TARGET): $(OBJS)
  $(CC) $(CFLAGS) -o $(TARGET) $(OBJS)

# ".PHONY" tells Make that "clean" is not a file
.PHONY: clean
clean:
  rm -f $(TARGET) $(OBJS)
\end{lstlisting}

Un ejemplo muy típico de compilación de un programa escrito en \texttt{C} sería el que se puede observar en el Listing \ref{lst:make}.

\subsubsection*{Just}

En este trabajo se utiliza una herramienta de construcción más nueva y polivalente llamada \texttt{just}\cite{just}. Este software tiene la misma finalidad que \texttt{make}, ejecutar comandos específicos de un proyecto. Pero este incluye muchas más funcionalidades, entre las cuales destacan:

\begin{itemize}
  \item Poder pasar parámetros a las ``recetas'' (las ``reglas'' en \texttt{make}).
  \item Crear aliases para las recetas.
  \item Cargar archivos \texttt{.env}.
  \item Poder definir recetas como scripts en el lenguaje que se prefiera, simplemente añadiendo el \textit{shebang}\cite{shebang} correspondiente.
  \item Ser capaz de ser invocado desde cualquier subdirectorio.
\end{itemize}

\begin{lstlisting}[language=make,label=lst:just]{Extracto de justfile utilizado en el proyecto}
# --- ALIASES ---
# Defines shortcuts (aliases) for longer commands.
alias dv := down_vol

# --- DEFAULT RECIPE ---
# This is the recipe that runs if you just type 'just' in the
# terminal.
# By default, it invokes the 'just -l' recipe, which lists all
# available recipes.
# The '_' prefix indicates that it is a helper recipe, not
# intended to be called directly by the user.
_default:
  just -l

# --- INTERNAL (PRIVATE) RECIPES ---
_build_zoo_base:
  #!/usr/bin/env bash
  if [[ "$(docker images -f reference=zoo-base | wc -l | xargs)" != "2" ]]
  then
    docker build --target base -t zoo-base .
  fi

# Accepts two parameters: 'entrypoint' and 'command'.
_run entrypoint command:
  # '@' at the beginning of a command line prevents 'just' from
  # printing the command before executing it.
  @just _build_zoo_base
  docker run --rm -w /app -v $PWD:/app --env-file .env --entrypoint={{entrypoint}} zoo-base {{command}}

# --- PUBLIC RECIPES ---
init:
  @just _run "yarn" "install"

down_vol:
  docker compose down -v
\end{lstlisting}

Como se puede comprobar en el Listing \ref{lst:just}, el archivo de configuración de \texttt{just}, en este caso nombrado habitualmente \texttt{justfile}, tiene una estructura similar a la de un \texttt{Makefile}. La diferencia principal es que los nombres de las recetas no hacen referencia a un archivo objetivo que se supone que se debe crear al ejecutar el bloque de comandos, sino que se trata simplemente del nombre de la receta.

\subsubsection*{Docker}

Docker\cite{docker} es una herramienta que permite empaquetar aplicaciones, creando imágenes con las dependencias necesarias  para que la aplicación se lance sin problemas. Las imágenes generadas se pueden ejecutar, creando contenedores, que son entornos completamente aislados del contexto de la máquina en la que han levantado. Estos contenedores son muy ligeros en cuanto a espacio y uso de recursos, ya que almacenan únicamente lo necesario para correr el software que queremos desplegar.

La mayor ventaja que proporciona el empaquetado de aplicaciones con Docker, es la portabilidad. Aunque hay que tener en cuenta la arquitectura de la máquina, las imágenes se pueden lanzar en cualquier entorno con Docker instalado, lo cual evita el conocido problema de: ``En mi máquina funciona''.

\begin{lstlisting}[language=Dockerfile,label=lst:dockerfile]{Extracto de Dockerfile utilizado en el proyecto}
# Base

FROM node:20 AS base

WORKDIR /app

COPY package.json lerna.json yarn.lock* ./

COPY packages packages/

RUN yarn install

RUN yarn global add lerna@8.2.1

RUN yarn global add @vercel/ncc

# Frontend build stage

FROM base AS frontend-build

WORKDIR /app

RUN lerna run --scope @vieites-tfg/zoo-frontend build 

# Frontend

FROM nginx:alpine AS frontend

WORKDIR /usr/share/nginx/html

COPY --from=frontend-build /app/packages/frontend/dist .

EXPOSE 80

CMD ["nginx", "-g", "daemon off;"]
\end{lstlisting}

El proceso más habitual a la hora de construir una imagen de Docker es definir un \texttt{Dockerfile} como el del Listing \ref{lst:dockerfile}. En este se indica, paso a paso, todo el proceso de instalación de dependencias y compilación del código fuente, necesario para lanzar la aplicación. En el \texttt{Dockerfile} mencionado, se puede observar que, además, se hace uso de \textit{multi-stage builds}, distintos estados de la construcción. Esto permite construir imágenes de Docker más pequeñas y optimizadas separando el proceso de construcción en distintas fases.

Una vez definido el \texttt{Dockerfile}, se puede construir y ejecutar la imagen con la secuencia de comandos del Listing \ref{lst:dockerbuildrun}

\begin{lstlisting}[language=bash,label=lst:dockerbuildrun]{Construir y correr una imagen de Docker}
docker build -t my-image .

docker run --rm -d -p 8080:80 my-image
\end{lstlisting}

El comando de la línea 1 construye la imagen que se define en el \texttt{Dockerfile} del directorio de trabajo actual (\texttt{.}), con el nombre \texttt{my-image}. Con el siguiente comando se ejecuta la imagen. Las \textit{flags} indican:

\begin{itemize}
  \item \texttt{--rm}

    Se eliminará el contenedor creado al finalizar su ejecución.

  \item \texttt{-d}

    El contenedor correrá en \textit{background}.

  \item \texttt{-p 8080:80}

    Se mapea el puerto 8080 de la máquina local al puerto 80 del contenedor.
\end{itemize}

\subsection*{Plataformas de despliegue}

\subsubsection*{Docker Compose}

Con Docker se es capaz de gestionar varios servicios desplegados en distintos contenedores. Pero existe una herramienta que apareció poco después y que facilita esta tarea, llamada ``Docker Compose''\cite{docker-compose}. Esta permite simular entornos con múltiples contenedores para desarrollar localmente.

\begin{lstlisting}[language=docker-compose,label=lst:dockercompose]{docker-compose.yaml usado en el proyecto}
services:
  zoo-frontend:
    image: ghcr.io/vieites-tfg/zoo-frontend
    container_name: zoo-frontend
    hostname: zoo-frontend
    ports:
      - "8080:80"
    depends_on:
      - zoo-backend
    environment:
      NODE_ENV: production
      YARN_CACHE_FOLDER: .cache

  zoo-backend:
    image: ghcr.io/vieites-tfg/zoo-backend
    container_name: zoo-backend
    hostname: zoo-backend
    ports:
      - "3000:3000"
    depends_on:
      - mongodb
    environment:
      NODE_ENV: production
      YARN_CACHE_FOLDER: .cache
      MONGODB_URI: "mongodb://${MONGO_ROOT}:${MONGO_ROOT_PASS}@mongodb:${MONGO_PORT:-27017}/${MONGO_DATABASE}?authSource=admin"

  mongodb:
    image: mongo:7.0
    container_name: zoo-mongo
    hostname: mongodb
    environment:
      - MONGO_INITDB_DATABASE=${MONGO_DATABASE}
      - MONGO_INITDB_ROOT_USERNAME=${MONGO_ROOT}
      - MONGO_INITDB_ROOT_PASSWORD=${MONGO_ROOT_PASS}
    ports:
      - "${MONGO_PORT_HOST:-27017}:${MONGO_PORT:-27017}"
    volumes:
      - ./mongo-init/:/docker-entrypoint-initdb.d/
      - mongo_data:/data/db

volumes:
  mongo_data:
\end{lstlisting}

En el archivo que se muestra en el Listing \ref{lst:dockercompose}, se puede observar cómo se configura el despliegue de tres servicios diferentes. Cada uno de los servicios se construye a partir de una imagen de Docker. Las imágenes correspondientes al frontend y al backend de la aplicación (\texttt{zoo-frontend} y \texttt{zoo-backend}, respectivamente) se generan y almacenan en un registro de GitHub al finalizar el ciclo de CI. Una vez publicadas, se pueden descargar indicando en el campo \texttt{image} el registro en el que están almacenadas junto con su nombre, como se puede ver en las líneas 4 y 16.

Simplemente, utilizando el comando del Listing \ref{lst:dcup}:

\begin{itemize}
  \item Se levantan los tres servicios.
  \item Se les pasarán las variables de entorno indicadas.
  \item Se podrá acceder a ellos a través de los puertos establecidos, siendo el primer número el puerto local y el segundo el puerto del contenedor (\texttt{<local>:<contenedor>}).
  \item Se compartirán los volúmenes mencionados.
\end{itemize}

\begin{lstlisting}[language=bash,label=lst:dcup]{Despliegue con Docker Compose}
docker compose up
\end{lstlisting}

Los valores de las variables de entorno, los indicados como \texttt{\$\{variable\}}, se obtienen de un archivo \texttt{.env}, el cual debe estar presente en el mismo directo que el archivo de configuración. En otro caso, es posible indicar la ruta al archivo mediante el campo \texttt{env\_file}, dentro de cada uno de los servicios configurados.

Docker Compose no es una plataforma de producción, se utiliza únicamente con el fin de desarrollar localmente, y es muy útil en el caso de querer hacer pruebas rápidas de una aplicación sencilla. Se puede tomar como un precursor conceptual a la orquestación más compleja que realiza Kubernetes\ref{subsec:k8shelm}.

\subsection*{Kubernetes \& Helm}
\label{subsec:k8shelm}

El proyecto Kubernetes nació un año después que Docker. Es una herramienta de software que permite orquestar contenedores. Permite gestionar el ciclo de vida de las aplicaciones en contenedores que viven en un \textit{cluster}. Entre sus características principales destacan:

\begin{itemize}
  \item Escalado automático.

    Aumenta o disminuye automáticamente el número de contenedores en ejecución. Esto va a depender de la cantidad de réplicas de una misma aplicación que se hayan indicado en su configuración. Kubernetes siempre va a intentar mantener el estado del \textit{cluster} cumpliendo los parámetros que se indicaron en las plantillas de configuración de cada uno de los servicios.

  \item Autorreparación.

    Si un contenedor falla, se reinicia o se reemplaza por otra instancia del mismo servicio, garantizando la continuidad de este.

  \item Descubrimiento de servicios y balanceo de carga.

    Se exponen los contenedores entre ellos y/o a Internet. Además, permite distribuir el tráfico de red, evitando así sobrecargas.
\end{itemize}

\begin{figure}
  \centerline{\includegraphics[width=10cm]{figuras/k8s}}
  \caption{Arquitectura de Kubernetes.\cite{img:k8s}}
  \label{fig:k8s}
\end{figure}

Un \textit{cluster} de Kubernetes se compone de dos tipos principales de servidores (``nodos''):

\begin{itemize}
  \item El \textit{Control Plane}.

    Toma todas las decisiones. Se encarga de que todo el sistema funcione como debe.

  \item Los nodos de trabajo.

    Donde realmente se ejecutan las aplicaciones. Reciben órdenes del \textit{Control Plane}. Puede y suele haber más de un nodo de trabajo en un \textit{cluster}.
\end{itemize}

Kubernetes te permite definir diferentes elementos en archivos \texttt{YAML}. Estos archivos describen el estado que deseamos que tenga el sistema en todo momento. Kubernetes se encarga de procesar estos archivos e intentar hacer que el estado real del sistema sea igual al estado deseado.

Entre los elementos que se pueden construir se encuentran los siguientes:

\begin{itemize}
  \item Pod.

    Es la unidad más pequeña que se puede crear. Puede tener uno o más contenedores, pero lo normal es que tenga solo uno. Su función es encapsular y ejecutar la aplicación que le corresponda, que se indica mediante una imagen de Docker.
  \item Deployment.

    Se trata de un controlador de Pods. Normalmente se utiliza este tipo de elementos en vez de crear Pods directamente. Esto es porque le puedes indicar la cantidad de Pods (réplicas) que deseas que haya en todo momento en el sistema, y el \textit{deployment} lo hace por ti.
  \item Service.

    Debido a que los Pods son efímeros, es decir, se crean y se destruyen constantemente, cambiando así su dirección IP; es necesario tener un elemento que funcione como punto fijo de acceso a un Pod. Para eso sirve un Service. Estos proporcionan un nombre y una IP únicos y fijos para los Pods.
  \item Ingress.

    Es un elemento más avanzado que un Service. Permite gestionar el acceso desde Internet, dirigiendo las peticiones hacia los servicios correctos dentro del \textit{cluster}.

  \item ConfigMap.

    Este elemento está diseñado para almacenar valores no sensibles. Se definen en formato clave-valor. Permite separar la configuración de una aplicación de su código.

  \item Secret.

    Es muy similar a un ConfigMap, pero está diseñado para almacenar y gestionar información sensible. Su función es guardar datos que no se deberían mostrar a simple vista en la configuración de una aplicación, como contraseñas o \textit{tokens} de autenticación.
\end{itemize}

Complementando a Kubernetes tenemos Helm, que se trata de un gestor de paquetes para Kubernetes. Su propósito es ayudar a instalar y administrar el ciclo de vida de las aplicaciones de Kubernetes. Además, las Helm Charts son formatos de archivos YAML que te permiten definir los objetos de Kubernetes de una manera dinámica. Esto te permite definir aplicaciones mucho más complejas.

\begin{figure}
  \centerline{\includegraphics[width=10cm]{figuras/helm}}
  \caption{Arquitectura de Helm.\cite{img:helm}}
  \label{fig:helm}
\end{figure}

\begin{lstlisting}[language=helm]{Ejemplo de objeto Ingress utilizado en este proyecto.}
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: {{ .Release.Name }}-{{ .Chart.Name }}-ingress
spec:
  rules:
    - host: {{ tpl .Values.ingress.hostTemplate . }}
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: {{ .Release.Name }}-{{ .Chart.Name }}-svc
                port:
                  number: {{ .Values.service.port }}
\end{lstlisting}

\section{Dagger}

\begin{figure}
  \centerline{\includegraphics[width=10cm]{figuras/dagger}}
  \caption{\textit{pipelines} con Dagger sobre un \textit{runtime} compatible con Docker.\cite{img:dagger}}
  \label{fig:dagger}
\end{figure}

Dagger es el pilar fundamental de este trabajo. Se trata de un kit de desarrollo de software que permite a los desarrolladores crear \textit{pipelines} CI/CD, y ejecutarlos en cualquier sitio.

La principal idea de los creadores de Dagger siempre ha sido poder crear \textit{pipelines} portables, que no sea necesario implementarlos de nuevo cada cierto tiempo debido a cambios en el entorno de desarrollo o de pruebas. Esta portabilidad se consigue utilizando cualquier \textit{runtime} de Docker para ejecutar las funciones que forman parte del SDK\cite{sdk} de Dagger, así como las que definen los propios desarrolladores. Además, desde el principio se ha evitado el uso de archivos YAML, que está siendo el lenguaje de configuración más utilizado por parte de la mayoría de aplicaciones.

\subsubsection*{CUE}

Dagger comenzó utilizando un lenguaje de configuración muy potente llamado CUE\cite{cue}. Este se podría ver como una extensión de JSON, pero con más funcionalidades. Todo lo escrito en JSON se puede traducir a CUE, pero no al revés. En el Listing \ref{lst:cue} se puede ver un ejemplo de código de Dagger en el que se utiliza CUE para lanzar un ``plan''. En él se descarga la imagen de Docker de Alpine y se almacena en un registro levantado en la máquina local.

\begin{lstlisting}[language=go,label=lst:cue]{Código de Dagger con CUE}
package main

import (
    "dagger.io/dagger"
    "universe.dagger.io/docker"
)

dagger.#Plan & {
    actions: {
        pull: docker.#Pull & {
            source: "alpine"
        }
        push: docker.#Push & {
            image: pull.output
            dest:  "localhost:5042/alpine"
        }
    }
}
\end{lstlisting}

Esta es la primera vez que se pueden ejecutar \textit{pipelines} definidas de manera programática, ejecutadas sobre un \textit{runtime} de Docker, y de manera relativamente funcional. Además, se puede ejecutar toda la secuencia de acciones de manera local, permitiendo realizar pruebas y buscar errores sin necesidad de hacer uso de otras herramientas de CI como GitHub Actions.

Además, Dagger hace un uso exhaustivo de la caché. Todas las acciones son cacheadas automáticamente. Esto es una funcionalidad muy útil, ya que va a hacer que un \textit{pipeline} se ejecute hasta un 90\% más rápido, como ha ocurrido en pruebas realizadas en este trabajo, que se comentan más adelante. Si se pone como ejemplo la ejecución de tests sobre una aplicación, la primera vez que se lanza el \textit{pipeline}, este tiene que ejecutarse completamente:

\begin{enumerate}
  \item Instalar las dependencias.
  \item Compilar la aplicación.
  \item Correr los tests.
\end{enumerate}

Dependiendo del tipo de aplicación y, evidentemente, de la conexión a Internet, esto puede tardar desde unos segundos hasta varios minutos. Pero con Dagger solo ocurre una vez. La primera vez. Gracias a la caché, todas las acciones repetitivas, como la instalación de dependencias o la compilación de la aplicación, se almacenan en la caché, ahorrando así mucho tiempo a la hora de realizar pruebas de cualquier tipo. Mientras tanto, con otras herramientas se tendría que esperar siempre la misma cantidad de tiempo para cada una de las veces que se quiere lanzar el \textit{pipeline}. Y, además, muchas veces ni siquiera serda de manera local, habría que depender de aplicaciones que pueden fallar o no estar disponibles en algún momento.

\subsubsection*{CI/CD como código}

Dagger a querido ir más allá pensando que los desarrolladores deberían ser capaces de crear sus \textit{pipelines} de la misma manera que crean sus aplicaciones, escribiendo código. Es así como el SDK de Go\cite{go} para Dagger, el cual se utiliza en este trabajo. Go es un lenguaje de programación con muchos casos de uso. Desde la creación de servicios de red y en la nube, hasta aplicaciones CLI y desarrollo web. Es conocido también por su simplicidad en cuanto a sintaxis y por tener una librería estándar muy completa, aportando muchas de las herramientas necesarias para realizar proyectos comunes. Además, facilita la implementación de programación concurrente, gracias a las \textit{goroutines}, similares a los hilos de ejecución de un sistema operativo, pero mucho más ligeros. En el Listing \ref{lst:daggergo} se puede observar un ejemplo de código\cite{daggergo} utilizando el SDK de Go.

\begin{lstlisting}[language=go,label=lst:daggergo]{Primeros ejemplos del SDK de Go de Dagger.}
package main

import (
  "context"
  "fmt"
  "os"

  "dagger.io/dagger"
)

func main() {
  err := doCi()
  if err != nil {
    fmt.Println(err)
  }
}

func doCi() error {
  ctx := context.Background()

  // create a Dagger client
  client, err := dagger.Connect(ctx, dagger.WithLogOutput(os.Stdout))
  if err != nil {
    return err
  }
  defer client.Close()

  src := client.Host().Directory(".") // get the projects source directory

  yarn := client.Container().From("yarnpkg/node-yarn"). // initialize new container from yarn image
  WithDirectory("/src", src).WithWorkdir("/src"). // mount source directory to /src
  WithExec([]string{"yarn", "test"}) // execute yarn test command

  // get test output
  test, err := yarn.Stdout(ctx)
  if err != nil {
    return err
  }
  // print output to console
  fmt.Println(test)

  // execute build command and get build output
  build, err := yarn.WithExec([]string{"yarn", "build"}).Stdout(ctx)
  if err != nil {
    return err
  }
  // print output to console
  fmt.Println(build)

  return nil
}
\end{lstlisting}

Todo lo anterior hace de Go una elección excelente para empezar la lista de lenguajes de programación sobre los que el equipo de Dagger implementaría su propio SDK, que hasta el día de hoy incluyen: Java, PHP, Node y Python. 

\subsubsection*{GraphQL}

Pero, ¿cómo es capaz el equipo de Dagger de desarrollar SDKs específicos para cada lenguaje tan rápidamente? Gracias al uso de GraphQL\cite{graphql}, un lenguaje para manipulación y consulta de datos.

\begin{figure}
  \centerline{\includegraphics[width=14cm]{figuras/graphql}}
  \caption{Uso de la API de GraphQL para Dagger.\cite{img:graphql}}
  \label{fig:graphql}
\end{figure}

Se puede ver su funcionamiento en la Figura \ref{fig:graphql}. El SDK de cada uno de los lenguajes funcionan como traductores del código que escribes en dicho lenguaje a sentencias que entiende el motor de Dagger. Esto es a través de la API de GraphQL de Dagger. El motor de Dagger es el que se encarga de ejecutar las instrucciones en un entorno controlado. De esta manera, no son los propios SDKs los que corren los programas dependiendo del lenguaje, sino que funcionan como clientes de la API para traducir la secuencia de acciones y ejecutarse en un mismo entorno.

\subsubsection*{Dagger \textit{functions}}

Entonces, tras varias mejoras y nuevas versiones, el equipo de Dagger implementó las ``funciones de Dagger''. Estas funciones son el componente principal de Dagger hoy en día. Cada una de las operaciones principales de Dagger se pueden llamar a través de una función, utilizando una API. Además, estas se pueden encadenar, generando \textit{pipelines} dinámicas en una sola llamada. De esta manera, se puede decir finalmente que gracias a Dagger podemos programar nuestros ciclos CI/CD, en el lenguaje que queramos, dentro de los que están disponibles.

\begin{lstlisting}[language=go,label=lst:daggerfunctions]{Extracto de funciones de Dagger de este trabajo}
// Builds the frontend package, generating only one executable
// file and returns the container.
func (m *Frontend) Build(ctx context.Context) *dagger.Container {
  build := m.Base.
    WithWorkdir("/app").
    WithExec([]string{"lerna", "run", "--scope", "@vieites-tfg/zoo-frontend", "build"})

  return build
}

// Based on the build stage, gets the executable file and creates
// a ready to run container with Nginx and the port 80 exported.
func (m *Frontend) Ctr(ctx context.Context) *dagger.Container {
  build := m.Build(ctx)

  dist := build.Directory("/app/packages/frontend/dist")

  ctr := dag.
    Container().
    From("nginx:alpine").
    WithWorkdir("/usr/share/nginx/html").
    WithDirectory(".", dist).
    WithExposedPort(80).
    WithEntrypoint([]string{"nginx", "-g", "daemon off;"})

  return ctr
}
\end{lstlisting}

La existencia de las funciones de Dagger permite la creación de módulos, un conjunto de funciones que toman una entrada y producen una salida en concreto. Los módulos creados por la comunidad se pueden encontrar en el Daggerverse\cite{daggerverse}. Este es el lugar en el que se comparten módulos de Dagger, los cuales se pueden utilizar en el caso de que alguno sea necesario para el \textit{pipeline} que se quiera crear.

\subsubsection*{CLI}

Otra funcionalidad que tiene Dagger es su CLI\cite{cli} (\textit{Command-Line Interface}). A través de ella puedes llamar a funciones de módulos de Dagger, tanto de de tu sistema de archivos local como directamente de un repositorio de Git.

\begin{lstlisting}[language=bash,label=lst:callbackend]{Comando para lanzar el backend del proyecto}
dagger call --sec-env=file//../../.env backend service up --ports 3000:3000
\end{lstlisting}

De esta manera (Listing \ref{lst:callbackend}) es como se ejecutan las funciones que se definen en los módulos creados en este trabajo. Toda la documentación al respecto se puede encontrar en \textbf{CITAR APÉNDICE}.

\subsubsection*{Resumen}

Dagger es una herramienta revolucionaria que redefine la manera de crear \textit{pipelines} de CI/CD, permitiendo implementarlos como código y evitar que los desarrolladores tengan que lidiar con archivos de configuración estáticos como YAML. Ofrece SDKs en una variedad de lenguajes de programación, tales como Go, Python o Node, que actúan como clientes del motor central de Dagger.

Una de sus ventajas más significativas es su portabilidad, la cual se consigue al ejecutar todas las operaciones sobre un \textit{runtime} de Docker. De esta manera se garantiza que cualquier \textit{pipeline} definido con Dagger funcione de igual manera sin importar la máquina en la que se ejecuta.

Otro de sus pilares es la gestión que hace de la caché. Dagger cachea cada una de las acciones ejecutadas. Gracias a esto, tras la primera ejecución de un \textit{pipeline}, las siguientes ejecuciones son significativamente más rápidas. Así se reducen los tiempos de espera, permitiendo a los desarrolladores trabajar más rápido.

Finalmente, la evolución hacia las ``Dagger \textit{functions}'' y la creación de módulos permite un gran nivel de reutilización. Estos módulos, que pueden ser compartidos a través del Daggerverse, junto con su CLI para invocarlos, proponen una manera muy poderosa de crear \textit{pipelines} para construir, probar y desplegar aplicaciones.
